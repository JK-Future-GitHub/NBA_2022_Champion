{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pngs/nba_html_crawler.png)\n",
    "1. Define Foundation\n",
    "2. Get Season Urls (1/2)\n",
    "2. Get Season Urls (2/2)\n",
    "3. Save ALL Team Seasons Html Pages \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 1 Define Foundation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time as t\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_URL = r\"https://www.basketball-reference.com/\"\n",
    "SEASON_URL = r\"https://www.basketball-reference.com/leagues/\"\n",
    "\n",
    "DATA_PATH = r\"C:\\Users\\knaue\\Documents\\Data\\NBA\"\n",
    "SEASON_HTML_PATH = os.path.join(DATA_PATH, \"SEASON_HTML\")\n",
    "SEASON_PATH = os.path.join(DATA_PATH, \"Season_Urls.csv\")\n",
    "SEASON_DETAIL_PATH = os.path.join(DATA_PATH, \"Season_Detail_Urls.csv\")\n",
    "\n",
    "PARSER = 'lxml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some infomations are hidden in comments --> convert comments in normal html code \n",
    "def filter_out_comment(soup: BeautifulSoup) -> BeautifulSoup:\n",
    "    content = str(soup).replace('<!--', '')\n",
    "    content = content.replace('-->', '')\n",
    "    return BeautifulSoup(content, PARSER)\n",
    "\n",
    "def request_data(url: str, sleep_time_sec: float = 1.0, with_comment: bool = True) -> BeautifulSoup:\n",
    "    t.sleep(sleep_time_sec)\n",
    "    \n",
    "    if with_comment: \n",
    "        return BeautifulSoup(requests.get(url).content, PARSER)\n",
    "    return filter_out_comment(BeautifulSoup(requests.get(url).content, PARSER))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 2 Get Season Urls (1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\Season_Urls.csv\n"
     ]
    }
   ],
   "source": [
    "content = request_data(SEASON_URL, with_comment=False)\n",
    "content = content.find(\"table\", id=\"stats\")\n",
    "df = pd.read_html(str(content))[0]\n",
    "df = df.droplevel(0, axis=1)\n",
    "\n",
    "seasons = []\n",
    "for season in df['Season'].values:\n",
    "    season = content.find(text=season)\n",
    "    seasons.append(urljoin(SEASON_URL, season.parent['href']))\n",
    "    \n",
    "df['Url_Season_Summary'] = seasons  \n",
    "df.to_csv(SEASON_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"Saved to: \", SEASON_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 2 Get Season Urls (2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85/85...\n",
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\Season_Detail_Urls.csv\n"
     ]
    }
   ],
   "source": [
    "df_season_urls = pd.read_csv(SEASON_PATH, usecols=['Season', 'Champion', 'Lg', 'Url_Season_Summary'], encoding=\"utf-8-sig\")\n",
    "dfs = []\n",
    "i = 0\n",
    "\n",
    "for season, lg, champ, url in df_season_urls.values:\n",
    "    i += 1\n",
    "    sys.stdout.write(f\"\\r{i}/{len(df_season_urls)}...\")\n",
    "    \n",
    "    content = request_data(url, sleep_time_sec=4.0, with_comment=False)\n",
    "    table = content.find(\"table\", id=\"per_game-team\")\n",
    "    df = pd.read_html(str(table))[0]\n",
    "    df = df[:-1]\n",
    "    \n",
    "    df['Season'] = season\n",
    "    df['Lg'] = lg\n",
    "    df['Playoffs'] = df['Team'].str.contains(\"\\*\")\n",
    "    df['Team'] = df['Team'].str.replace(\"\\*\", \"\", regex=True)\n",
    "    \n",
    "    if champ is np.nan: \n",
    "        df['Champion'] = None     \n",
    "    else:    \n",
    "        df['Champion'] = df['Team'].str.contains(champ)\n",
    "    \n",
    "    df.drop([name for name in df.columns if name not in ['Team', 'Season', 'Lg', 'Playoffs', 'Champion']], axis=\"columns\", inplace=True)\n",
    "    df['Url_Season_Summary'] = url\n",
    "    sec_url = content.find(\"a\", text=\"Standings\")\n",
    "    df['Url_Season_Standings'] = urljoin(SEASON_URL, sec_url['href'])\n",
    "    df['Url_Playoff_Standings'] = df['Url_Season_Standings'].str.replace(\"leagues\", \"playoffs\")\n",
    "    \n",
    "    teams_url = []\n",
    "    for team in df['Team'].values:\n",
    "        team = content.find(text=team)\n",
    "        teams_url.append(urljoin(MAIN_URL, team.parent['href']))\n",
    "        \n",
    "    df['Url_Team_Season_Summary'] = teams_url  \n",
    "    dfs.append(df)\n",
    "    \n",
    "dfs = pd.concat(dfs, ignore_index=True)\n",
    "dfs.to_csv(SEASON_DETAIL_PATH, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"\\nSaved to: \", SEASON_DETAIL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ➤ 3 Save ALL Team Seasons Html Pages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228/228...\n",
      "Saved to:  C:\\Users\\knaue\\Documents\\Data\\NBA\\SEASON_HTML ...\n"
     ]
    }
   ],
   "source": [
    "df_season_detail_urls = pd.read_csv(SEASON_DETAIL_PATH, usecols=['Url_Season_Summary', 'Url_Season_Standings', 'Url_Playoff_Standings'], encoding=\"utf-8-sig\")\n",
    "i = 0\n",
    "\n",
    "unique_url_season_sum = df_season_detail_urls['Url_Season_Summary'].unique()\n",
    "unique_url_season_sta = df_season_detail_urls['Url_Season_Standings'].unique()\n",
    "unique_url_playoff_sta = df_season_detail_urls['Url_Playoff_Standings'].unique()\n",
    "unique_urls = np.concatenate((unique_url_season_sum, unique_url_season_sta, unique_url_playoff_sta), axis=0)\n",
    "\n",
    "for url in unique_urls:\n",
    "    i += 1\n",
    "    sys.stdout.write(f\"\\r{i}/{len(unique_urls)}...\")\n",
    "    \n",
    "    content = request_data(url=url, sleep_time_sec=4.0, with_comment=True)\n",
    "    url = url.replace(\"/\", \"{\").replace(\":\", \"}\")                           # Cant be saved the slash and : -> convert in /={ and :=}  \n",
    "    season_path = os.path.join(SEASON_HTML_PATH, url)\n",
    "    \n",
    "    with open(season_path, \"w\", encoding='utf-8-sig') as f:\n",
    "        f.write(str(content))\n",
    "        f.close()\n",
    "\n",
    "print(\"\\nSaved to: \", SEASON_HTML_PATH, \"...\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aa6708a3e9a8fadf3ed03c473ecc2d2a3bf5cea3ad7526930f095379c19fd7a0"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
